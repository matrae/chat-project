How would you create artificial intelligence to learn good from bad? Are you sure that training process isn't going on with humans in a virtual world?
First, you would need to define “good” and “bad”. This is harder than you might think, since our definition is more fluid than we’d like to admit. Most people’s definition is based on a number of factors:
The law, which codifies the rules of the society you life within.
Norms, which are common behaviors we share.
Biology, which informs our most primal values.
It would be easy to train an AI to obey #1, since the law is a series of documents. In fact, most law students are taught using a series of scenarios and their legal features which would be ideal training data for an AI system. Of course, as people we aren’t always sure what the law says (which is why we have courts) but you could train an AI to be at least as legal as a normal person.
It would be harder to obey #2, since there is no written definition of most norms. For example, when should you hold the door for someone else? When is it okay to speak loudly? While these sound like small issues, an AI ignorant of them would be considered extremely rude which might be construed as “bad”.
It would be almost impossible to train an AI for #3, but not for the reason you think. Yes, you can teach the AI to focus on self preservation, procreation and all of the common biological imperatives. However, how do you train it to decide when #3 overcomes #1? For example, when is it okay to kill in self defense? The interplay of all 3 becomes the biggest challenge in teaching “good” from “bad” and there is no simple set of scenarios that would help it.
I believe that to truly educate an AI on concepts of “good” and “bad” you would need to raise it similarly to how we raise children. The AI would start with a set of primitive rules (#3) and would be educated through a combination of teaching facts (#1) and watching us behave (#2). Some of the lessons might be contradictory, so there would need to be a feedback loop like a teacher or parent to help the AI with ambiguous problems.
It is exciting to think that we are not very far from a future where this might be possible. Self-driving cars are being taught in a similar way even today! They start with a set of rules, then learn from watching drivers drive. Then they learn by driving themselves and being corrected by a driver (teacher). In the end they can drive even better than the humans that developed them, even in a world of contradictory rules and ambiguous decisions.

Can an AI have ethics?
This is the most ‘important’ question surrounding Artificial Intelligence.
With the way we are progressing and aim to use AI in our social setup, the impact and implications are immense.
This is how the ‘Ethics’ came into picture and still is mostly under research and discussion.
Use of AI in military combat and as autonomous systems has been an area of concern to not just experts but layman as well.
As AI starts to become more intelligent and starts building its own perception, how it will perceive the social structure and us(humans) in this entire framework is something we must attend to at earliest.
AI can be biased, as much as we are, since the once developing it could impact its data feed and algorithm based on their mindset. So, ethics to AI is nothing but a set of code that lets it determine the appropriate behavior and reaction in a specified condition.
The appropriate question should be ‘Whether AI should have ethics?’.
As per Tim Cook, “If we do not design it responsibly, the implications of AI would be profound”.

Can moral intuition be mechanised or otherwise imitated by artificial intelligence?
Morality comes from moralis: (Latin for temperament), it’s derived from ethos which is (disposition and nature). Since morality is a human construct and really only applies to the relationship between TWO people; it is not ubiquitous as a fundamental aspect of humanity. There is no grounding in morality to have separate foundations that are emergent properties of human nature, however the reverse is true, there can only be acts which are immoral and by those a standard of moral behavior which doesn’t contain immoral aspects.
In order to mechanize abstract behavior you need to attribute some real objective classification towards actions and their abstract counterparts. ie classifiying good behavior aka classifying behavior conducive towards a fundamental goal everyone can participate in. (Notice everyone has to be included because that alleges sectional behavior can be counter moral and become antipathy)
ie is feeding everyone moral? Is saving lives moral? These questions and solutions need a pragmatic group consensus.
To mechanise moral intuition, there would have to be a general agreement on what moral is. Moral is based on many things from your personal experincese to the way society functions. There are general principals but even these can be broken when confict increases. Is one religion better than another, is it natural that some get better chances than others. Moral is attemped codified in national laws, but even these conflict sometimes.
And if it was that simple we would not need a juridical system with lawyers and courts.
Lie detectors are an attempt at that. “Moral intuition” though seems to imply being able to detect motive as well as truthfulness based on anxiety. This is where humans often fall short, not having enough information to see pattern and motive or even ask the right questions.
Can this be done better with the assistance of AI? Slowly a whole slew of pattern recognition has been developing for things like traffic management, facial and voice recognition, financial pattern analysis, writing and speech (vocabulary, rhythm, handwriting, punctuation) comparison, medical monitoring, etc, etc.
Theoretically a super “Big Brother” could put sufficient data together to detect personal aberration indicative of suspect negative behaviors.
On the other hand my great aunt is impossible to lie to and always knows when we are up to something. El Al airlines is famous for having such personnel as their primary security method, a bunch of Jewish aunts and uncles who walk the passenger lines, observe and ask questions, who are actually a hundred times better than all the trillions of dollars the USA has spent on detectors and grossly inefficient processes.
I’m fairly sure AI could do a decent job given enough access to data but as long as profit driven “defense” contractors are profiting so massively from both human and artificial stupidity we won’t get there. It’ll probably come about from another angle, not security or law enforcement related. There could be a lot of positive use for people monitoring their own medical, psychological, or behavior conditions for health and performance reasons (as athletes often adjust many factors throughout their lives during particular training cycles).

Could we actually make Asimov’s 3 laws of robotics work, i.e. be immutable concepts in every AI?
A robot may not harm any human being, nor by its inaction allow a human being to come to harm.
Problem with it, how do you define “harm”? And how does an AI know it is causing harm to someone?
Most AI programs which already exist, understand only their very specific domain, and most of them do not know what a “human being” is, let alone “harm”. And yet, some of these programs — the ones involved in stock trading come to mind, — are quite capable of causing unintended harm. Usually in the process of doing benefit to their clients.
Keep in mind, Three Laws of Robotics did not work very well even in Asimov’s stories — the plot of pretty much every story was how these laws lead to unexpected, and often harmful, results. And that’s despite the fact that Asimov’s robots had totally implausible degree of understanding what is and is not harm — often completely at odds with the rest of their cognitive capacities.

In your opinion, what ethics do we need to imprint in AI so that it will be as close to humanity without it being hostile? Is Asimov's law enough?
I believe that sometimes we tend to generate very biased assumptions about what expectations we relate to the role AI will serve our future…
Do we really want an AI to adopt human ethics in every aspect of humanity? Do we further believe that we will somehow pull the wool over the theoretical eyes of AI regarding the complete structural ethical behavior of humanity?
Or would we like AI to ascend our values and evaluate humanity from a higher plane of virtue?
We need to first take a really hard look at ourselves before approaching our collective expectations of AI…
Many people cannot see the value of a social security system until they see how regular people act financially when given full financial control. Many people see the practicality of contracting a reverse mortgage, until they observe the financial behavior of the average person who acquires one.
Can you see where this is going? Everyone has a different capacity for responsible behavior regardlessly of how it affects their future. Humans take risks, it's who we are collectively. However, sometimes we all need structure in some form. Who controls that structure is the fundamental question we need to address…
Humanity is self-destructive.
We need no help from AI to self destruct. How many people do you know that are seriously hungry or really know hunger? How often do you turn your head away from truly tragic situations across the world? Would we like an AI that theoretically turns it's head away from the hungry souls across the land?
Once we acknowledge the true nature of our self-destructiveness, we will truly understand the role that AI will serve…
Technological progress does not exactly translate in to the longevity of the human race, and until we address that, we will never understand what role AI should serve… We must understand ourselves first…or at least try…

What would guide the morals of an advanced free-willed humanoid AI? How would it decide on whether to be good or bad?
This question is not answerable for 2 reasons.
First, You're assuming that we have developed a level of understanding of intelligence that is simply not even considered in the current state of computing. We can't even agree on what is intelligence.
Second, we can't really describe why we have whatever morals we each manifest, nor have a firm understanding of how we can reliably pass them on to our own children
It depends on its individual motivations. We could say the same of most humans though. What do we strive for? Who are we willing to throw under the bus to get it? Generally (although not always), the greater the ambitions of an individual, the more they are willing to sacrifice to achieve it.
You can't really know what someone is going to do in any given situation until you understand what their long term goals and ambitions are, and even then sometimes those people can do things you'd never expect.
I think that a lot of our morals come from our culture and sociological influences. Without these influences, the AI would revert to its programming and all of its decisions would likely be based on the most logical way to achieve its programmed goals.
Preservation of innocent life, as guided by the ethical system of Prudence, which combines experience (i.e., programmed memory for AI) and practicality. Prudence advocates striving for ethical perfection, but recognizing that it is often impractical or even imprudent. Under those circumstances, recognizing that we are not perfect, we should still need to have expectations for appropriate behavior; which makes AI that much more difficult, if not impossible.
A principle is the expression of perfection, and as imperfect beings like us cannot practice perfection, we devise every moment limits of its compromise in practice.

